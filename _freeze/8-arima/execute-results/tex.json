{
  "hash": "a14a732a18b2bc0f94543507eaf9a329",
  "result": {
    "markdown": "---\ntitle: \"Time Series Analysis & Forecasting Using R\"\nsubtitle: \"8. ARIMA models\"\n---\n\n\n## Outline\n\n\\vspace*{0.7cm}\\tableofcontents\n\n\n\n\n\n\n# ARIMA models\n\n## ARIMA models\n\n\\begin{tabular}{rl}\n\\textbf{AR}: & autoregressive (lagged observations as inputs)\\\\\n\\textbf{I}: & integrated (differencing to make series stationary)\\\\\n\\textbf{MA}: & moving average (lagged errors as inputs)\n\\end{tabular}\n\n\\pause\n\n###\nAn ARIMA model is rarely interpretable in terms of visible data structures like trend and seasonality. But it can capture a huge range of time series patterns.\n\n## Stationarity\n\n\\begin{block}{Definition}\nIf $\\{y_t\\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\\dots,y_{t+s})$ does not depend on $t$.\n\\end{block}\\pause\n\nA **stationary series** is:\n\n* roughly horizontal\n* constant variance\n* no patterns predictable in the long-term\n\n## Stationary?\n\\fontsize{11}{12}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/unnamed-chunk-2_e96e39c58e03cd14a880dd6d59f5ff4f'}\n\n```{.r .cell-code}\ngafa_stock |>\n  filter(Symbol == \"GOOG\", year(Date) == 2018) |>\n  autoplot(Close) +\n  labs(y = \"Google closing stock price ($US)\")\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n## Stationary?\n\\fontsize{11}{12}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/unnamed-chunk-3_25ed144ec1d22fdae77e59f2e0b1a49d'}\n\n```{.r .cell-code}\ngafa_stock |>\n  filter(Symbol == \"GOOG\", year(Date) == 2018) |>\n  autoplot(difference(Close)) +\n  labs(y = \"Daily change in Google closing stock price\")\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/unnamed-chunk-3-1.pdf)\n:::\n:::\n\n\n## Differencing\n\\fontsize{13}{15}\\sf\n\n* Differencing helps to **stabilize the mean**.\n* The differenced series is the *change* between each observation in the original series.\n* Occasionally the differenced data will not appear stationary and it may be necessary to difference the data a second time.\n* In practice, it is almost never necessary to go beyond second-order differences.\n\n## Autoregressive models\n\n\\begin{block}{Autoregressive (AR) models:}\\vspace*{-0.3cm}\n$$\n  y_{t} = c + \\phi_{1}y_{t - 1} + \\phi_{2}y_{t - 2} + \\cdots + \\phi_{p}y_{t - p} + \\varepsilon_{t},\n$$\nwhere $\\varepsilon_t$ is white noise. A multiple regression with \\textbf{lagged values} of $y_t$ as predictors.\n\\end{block}\n\n\n::: {.cell hash='8-arima_cache/beamer/arp_125e3335057cd6616f5cd245ada2d8c6'}\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/arp-1.pdf)\n:::\n:::\n\n\n\\vspace*{-0.4cm}\n\n* Cyclic behaviour is possible when $p\\ge 2$.\n\n## Moving Average (MA) models\n\n\\begin{block}{Moving Average (MA) models:}\\vspace*{-0.3cm}\n$$\n  y_{t} = c + \\varepsilon_t + \\theta_{1}\\varepsilon_{t - 1} + \\theta_{2}\\varepsilon_{t - 2} + \\cdots + \\theta_{q}\\varepsilon_{t - q},\n$$\nwhere $\\varepsilon_t$ is white noise.\nA multiple regression with \\textbf{lagged \\emph{errors}} as predictors. \\emph{Don't confuse with moving average smoothing!}\n\\end{block}\n\n\n::: {.cell hash='8-arima_cache/beamer/maq_8c4e626e72d965f295b9fa91824ffa62'}\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/maq-1.pdf)\n:::\n:::\n\n\n## ARIMA models\n\n\\begin{block}{Autoregressive Moving Average models:}\\vspace*{-0.7cm}\n\\begin{align*}\n  y_{t} &= c + \\phi_{1}y_{t - 1} + \\cdots + \\phi_{p}y_{t - p} \\\\\n        & \\hspace*{2.4cm}\\text{} + \\theta_{1}\\varepsilon_{t - 1} + \\cdots + \\theta_{q}\\varepsilon_{t - q} + \\varepsilon_{t}.\n\\end{align*}\n\\end{block}\\pause\n\n* Predictors include both **lagged values of $y_t$ and lagged errors.**\n\\pause\n\n### Autoregressive Integrated Moving Average models\n* Combine ARMA model with **differencing**.\n* $d$-differenced series follows an ARMA model.\n* Need to choose $p$, $d$, $q$ and whether or not to include $c$.\n\n## ARIMA models\n\n\\begin{block}{ARIMA($p, d, q$) model}\n\\begin{tabular}{rl}\nAR:& $p =$ order of the autoregressive part\\\\\nI: & $d =$ degree of first differencing involved\\\\\nMA:& $q =$ order of the moving average part.\n\\end{tabular}\n\\end{block}\n\n* White noise model: ARIMA(0,0,0)\n* Random walk: ARIMA(0,1,0) with no constant\n* Random walk with drift: ARIMA(0,1,0) with \\rlap{const.}\n* AR($p$): ARIMA($p$,0,0)\n* MA($q$): ARIMA(0,0,$q$)\n\n## Example: National populations\n\\fontsize{11}{12}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/popfit2_f59e221a08723ef1fdb54521ef31738d'}\n\n```{.r .cell-code}\nfit <- global_economy |>\n  model(arima = ARIMA(Population))\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A mable: 263 x 2\n# Key:     Country [263]\n   Country                               arima\n   <fct>                               <model>\n 1 Afghanistan                  <ARIMA(4,2,1)>\n 2 Albania                      <ARIMA(0,2,2)>\n 3 Algeria                      <ARIMA(2,2,2)>\n 4 American Samoa               <ARIMA(2,2,2)>\n 5 Andorra             <ARIMA(2,1,2) w/ drift>\n 6 Angola                       <ARIMA(4,2,1)>\n 7 Antigua and Barbuda <ARIMA(2,1,2) w/ drift>\n 8 Arab World                   <ARIMA(0,2,1)>\n 9 Argentina                    <ARIMA(2,2,2)>\n10 Armenia                      <ARIMA(3,2,0)>\n# i 253 more rows\n```\n:::\n:::\n\n\n## Example: National populations\n\\fontsize{11}{12}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/popfit3_8913134bad38050b7a5947d072778173'}\n\n```{.r .cell-code}\nfit |>\n  filter(Country == \"Australia\") |>\n  report()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: Population \nModel: ARIMA(0,2,1) \n\nCoefficients:\n         ma1\n      -0.661\ns.e.   0.107\n\nsigma^2 estimated as 4.063e+09:  log likelihood=-699\nAIC=1401   AICc=1402   BIC=1405\n```\n:::\n:::\n\n\n\\only<2>{\\begin{textblock}{6.4}(6,4.6)\n\\begin{alertblock}{}\\fontsize{12}{13}\\sf\n\\centerline{$y_t = 2y_{t-1} - y_{t-2} - 0.7 \\varepsilon_{t-1} + \\varepsilon_t$}\n\\mbox{}\\hfill$\\varepsilon_t \\sim \\text{NID}(0,4\\times10^9)$\n\\end{alertblock}\n\\end{textblock}}\n\\vspace*{3cm}\n\n## Understanding ARIMA models\n\n* If $c=0$ and $d=0$, the long-term forecasts will go to zero.\n* If $c=0$ and $d=1$, the long-term forecasts will go to a non-zero constant.\n* If $c=0$ and $d=2$, the long-term forecasts will follow a straight line.\n* If $c\\ne0$ and $d=0$, the long-term forecasts will go to the mean of the data.\n* If $c\\ne0$ and $d=1$, the long-term forecasts will follow a straight line.\n* If $c\\ne0$ and $d=2$, the long-term forecasts will follow a quadratic trend.\n\n## Understanding ARIMA models\n\\fontsize{14}{15.5}\\sf\n\n### Forecast variance and $d$\n  * The higher the value of $d$, the more rapidly the prediction intervals increase in size.\n  * For $d=0$, the long-term forecast standard deviation will go to the standard deviation of the historical data.\n\n## Example: National populations\n\\fontsize{9}{9}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/popfc2_928eaa7612087ad41c113775cfcbaa6a'}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 10) |>\n  filter(Country == \"Australia\") |>\n  autoplot(global_economy)\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/popfc2-1.pdf)\n:::\n:::\n\n\n## How does ARIMA() work?\n\n\\begin{alertblock}{Hyndman and Khandakar (JSS, 2008) algorithm:}\n\\begin{itemize}\\tightlist\n\\item Select no.\\ differences $d$ via KPSS test.\n\\item Select $p$, $q$ and inclusion of $c$ by minimising AICc.\n\\item Use stepwise search to traverse model space.\n\\end{itemize}\n\\end{alertblock}\\pause\n\n\\begin{block}{}\n\\centerline{$\\displaystyle \\text{AICc} = -2 \\log(L) + 2(p+q+k+1)\\left[1 + \\frac{(p+q+k+2)}{T-p-q-k-2}\\right]$}\nwhere $L$ is the maximised likelihood fitted to the \\textit{differenced} data,\n$k=1$ if $c\\neq 0$ and $k=0$ otherwise.\\pause\n\\end{block}\n\nNote: Can't compare AICc for different values of $d$.\n\n## How does ARIMA() work?\n\\fontsize{12.5}{14.5}\\sf\n\nStep1:\n: Select current model (with smallest AICc) from:\\newline\nARIMA$(2,d,2)$\\newline\nARIMA$(0,d,0)$\\newline\nARIMA$(1,d,0)$\\newline\nARIMA$(0,d,1)$\n\\pause\\vspace*{-0.1cm}\n\nStep 2:\n: Consider variations of current model:\n\n    * vary one of $p,q,$ from current model by $\\pm1$;\n    * $p,q$ both vary from current model by $\\pm1$;\n    * Include/exclude $c$ from current model.\n\n  Model with lowest AICc becomes current model.\n\n\\pause\\alert{Repeat Step 2 until no lower AICc can be found.}\n\n## How does ARIMA() work?\n\n\n::: {.cell hash='8-arima_cache/beamer/ARMAgridsearch_8e9917b8460432fe8b0c609fada308cc'}\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/ARMAgridsearch-1.pdf){width=60%}\n:::\n:::\n\n\n## How does ARIMA() work?\n\n\n::: {.cell hash='8-arima_cache/beamer/ARMAgridsearch2_d00bdfbe555570e93dc2f82eabe640cc'}\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/ARMAgridsearch2-1.pdf){width=60%}\n:::\n:::\n\n\n## How does ARIMA() work?\n\n\n::: {.cell hash='8-arima_cache/beamer/ARMAgridsearch3_1b59ab04a91f872dcf9e44c74840f8bd'}\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/ARMAgridsearch3-1.pdf){width=60%}\n:::\n:::\n\n\n## How does ARIMA() work?\n\n\n::: {.cell hash='8-arima_cache/beamer/ARMAgridsearch4_bee4ce0893348ec8d015fd59d33266eb'}\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/ARMAgridsearch4-1.pdf){width=60%}\n:::\n:::\n\n\n# Lab Session 16\n## Lab Session 16\n\nFor the United States GDP data (from `global_economy`):\n\n * Fit a suitable ARIMA model for the logged data.\n * Produce forecasts of your fitted model. Do the forecasts look reasonable?\n\n# Seasonal ARIMA models\n\n## Seasonal ARIMA models\n\\fontsize{13}{15}\\sf\n\n| ARIMA | $~\\underbrace{(p, d, q)}$ | $\\underbrace{(P, D, Q)_{m}}$ |\n| ----: | :-----------------------: | :--------------------------: |\n|       | ${\\uparrow}$              | ${\\uparrow}$                 |\n|       | Non-seasonal part         | Seasonal part of             |\n|       | of the model              | of the model                 |\n\n\\vspace*{-0.4cm}\n\n  * $m =$ number of observations per year.\n  * $d$ first differences, $D$ seasonal differences\n  * $p$ AR lags, $q$ MA lags\n  * $P$ seasonal AR lags, $Q$ seasonal MA lags\n\n###\nSeasonal and non-seasonal terms combine multiplicatively\n\n## Cortecosteroid drug sales\n\n\n::: {.cell hash='8-arima_cache/beamer/unnamed-chunk-4_824687941d47e94c55f9b84fd61995bd'}\n\n```{.r .cell-code}\nh02 <- PBS |>\n  filter(ATC2 == \"H02\") |>\n  summarise(Cost = sum(Cost) / 1e6)\n```\n:::\n\n\n## Cortecosteroid drug sales\n\n\n::: {.cell hash='8-arima_cache/beamer/unnamed-chunk-5_d1d4aa5b239a9bf5bdcc3e4f5d6eb876'}\n\n```{.r .cell-code}\nh02 |> autoplot(\n  Cost\n)\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/unnamed-chunk-5-1.pdf)\n:::\n:::\n\n\n## Cortecosteroid drug sales\n\n\n::: {.cell hash='8-arima_cache/beamer/unnamed-chunk-6_e1a871b07f2804c37b192b9d8d001998'}\n\n```{.r .cell-code}\nh02 |> autoplot(\n  log(Cost)\n)\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/unnamed-chunk-6-1.pdf)\n:::\n:::\n\n\n## Cortecosteroid drug sales\n\n\n::: {.cell hash='8-arima_cache/beamer/unnamed-chunk-7_82b7c05f17009a3f433bc279f33fde2c'}\n\n```{.r .cell-code}\nh02 |> autoplot(\n  log(Cost) |> difference(12)\n)\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/unnamed-chunk-7-1.pdf)\n:::\n:::\n\n\n## Cortecosteroid drug sales\n\n\n::: {.cell hash='8-arima_cache/beamer/unnamed-chunk-8_4e68ff595a152c73a5d1d48c8147bffc'}\n\n```{.r .cell-code}\nh02 |> autoplot(\n  log(Cost) |> difference(12) |> difference(1)\n)\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n## Example: US electricity production\n\\fontsize{11}{12}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/h02fit_379ff7140e1508ec53438fdf815cf334'}\n\n```{.r .cell-code}\nh02 |>\n  model(arima = ARIMA(log(Cost))) |>\n  report()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: Cost \nModel: ARIMA(2,1,0)(0,1,1)[12] \nTransformation: log(Cost) \n\nCoefficients:\n          ar1      ar2     sma1\n      -0.8491  -0.4207  -0.6401\ns.e.   0.0712   0.0714   0.0694\n\nsigma^2 estimated as 0.004387:  log likelihood=245\nAIC=-483   AICc=-483   BIC=-470\n```\n:::\n:::\n\n\n## Example: US electricity production\n\\fontsize{11}{13}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/h02fcst_aefcc92292537de418645a705826950d'}\n\n```{.r .cell-code}\nh02 |>\n  model(arima = ARIMA(log(Cost))) |>\n  forecast(h = \"3 years\") |>\n  autoplot(h02)\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/h02fcst-1.pdf)\n:::\n:::\n\n\n\\vspace*{5cm}\n\n## Cortecosteroid drug sales\n\\fontsize{9}{9}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/h02tryharder_0ca26158aed2ef42ecdea2bcb4f7fb0d'}\n\n```{.r .cell-code}\nfit <- h02 |>\n  model(best = ARIMA(log(Cost),\n    stepwise = FALSE,\n    approximation = FALSE,\n    order_constraint = p + q + P + Q <= 9\n  ))\nreport(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: Cost \nModel: ARIMA(4,1,1)(2,1,2)[12] \nTransformation: log(Cost) \n\nCoefficients:\n          ar1    ar2    ar3     ar4     ma1   sar1    sar2    sma1   sma2\n      -0.0425  0.210  0.202  -0.227  -0.742  0.621  -0.383  -1.202  0.496\ns.e.   0.2167  0.181  0.114   0.081   0.207  0.242   0.118   0.249  0.213\n\nsigma^2 estimated as 0.004049:  log likelihood=254\nAIC=-489   AICc=-487   BIC=-456\n```\n:::\n:::\n\n\n## Cortecosteroid drug sales\n\\fontsize{11}{14}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/h02f_222487d6f31bdf65070e880b358b21fa'}\n\n```{.r .cell-code}\nfit |>\n  forecast() |>\n  autoplot(h02) +\n  labs(y = \"H02 Expenditure ($AUD)\", x = \"Year\")\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/h02f-1.pdf)\n:::\n:::\n\n\n# Lab Session 17\n## Lab Session 17\n\nFor the Australian tourism data (from `tourism`):\n\n * Fit a suitable ARIMA model for all data.\n * Produce forecasts of your fitted models.\n * Check the forecasts for the \"Snowy Mountains\" and \"Melbourne\" regions. Do they look reasonable?\n\n# Forecast ensembles\n\n## Forecast ensembles\n\\fontsize{10}{11}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/trainall_f3cd3c1c0f43721486424c0c8f62a69b'}\n\n```{.r .cell-code}\ntrain <- tourism |>\n  filter(year(Quarter) <= 2014)\nfit <- train |>\n  model(\n    ets = ETS(Trips),\n    arima = ARIMA(Trips),\n    snaive = SNAIVE(Trips)\n  ) |>\n  mutate(mixed = (ets + arima + snaive) / 3)\n```\n:::\n\n\n\\fontsize{13}{14}\\sf\n\n * Ensemble forecast `mixed` is a simple average of the three fitted models.\n *  `forecast()` will produce distributional forecasts taking into account the correlations between the forecast errors of the component models.\n\n## Forecast ensembles\n\\fontsize{10}{11}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/trainfc_a72ebf800626d439f0697daa7876d842'}\n\n```{.r .cell-code}\nfc <- fit |> forecast(h = \"3 years\")\nfc |>\n  filter(Region == \"Snowy Mountains\", Purpose == \"Holiday\") |>\n  autoplot(tourism, level = NULL)\n```\n\n::: {.cell-output-display}\n![](8-arima_files/figure-beamer/trainfc-1.pdf)\n:::\n:::\n\n\n## Forecast ensembles\n\\fontsize{10}{11}\\sf\n\n\n::: {.cell hash='8-arima_cache/beamer/snowy-test-accuracy_bb273708ccfd78ded26cc790c4b68058'}\n\n```{.r .cell-code}\naccuracy(fc, tourism) |>\n  group_by(.model) |>\n  summarise(\n    RMSE = mean(RMSE),\n    MAE = mean(MAE),\n    MASE = mean(MASE)\n  ) |>\n  arrange(RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 4\n  .model  RMSE   MAE  MASE\n  <chr>  <dbl> <dbl> <dbl>\n1 mixed   19.8  16.0 0.997\n2 ets     20.2  16.4 1.00 \n3 snaive  21.5  17.3 1.17 \n4 arima   21.9  17.8 1.06 \n```\n:::\n:::\n\n\n## Forecast ensembles\n\\fontsize{13}{15}\\sf\n\n\\begin{alertblock}{}\\bfseries Can we do better than equal weights?\\end{alertblock}\\pause\\vspace*{-0.3cm}\n\n * Hard to find weights that improve forecast accuracy.\n * Known as the \"forecast combination puzzle\".\n * Solution:  FFORMA\n\n\\pause\n\n\\begin{block}{\\fontsize{13}{14}\\sf\\textbf{FFORMA (Feature-based FORecast Model Averaging)}}\n\\begin{itemize}\n\\item Vector of time series features used to predict best weights.\n\\item A modification of xgboost is used.\n\\item Method came 2nd in the 2018 M4 international forecasting competition.\n\\item Main author: Pablo Montero-Manso (now Uni Sydney)\n\\item Not (yet) available for fable.\n\\end{itemize}\n\\end{block}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}