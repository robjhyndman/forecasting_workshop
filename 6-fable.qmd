---
title: "Time Series Analysis & Forecasting Using R"
subtitle: "6. Introduction to forecasting"
---

## Outline

\vspace*{0.7cm}\tableofcontents


```{r}
#| label: setup
#| include: false
#| cache: false
source("setup.R")
h02 <- tsibbledata::PBS |>
  filter(ATC2 == "H02") |>
  summarise(Cost = sum(Cost))
melsyd <- tsibbledata::ansett |>
  filter(Airports == "MEL-SYD")
```

# Statistical forecasting

## Forecasting is difficult

\full{hopecasts2}

## What can we forecast?

\full{nasdaq-stock-market}

## What can we forecast?

\full{Forex2}

## What can we forecast?

\full{pills}

## What can we forecast?

\full{elecwires2}

## What can we forecast?

\full{AusBOM}

## What can we forecast?

\full{ts22015}

## What can we forecast?

\full{comet}

## Which is easiest to forecast?

 1. daily electricity demand in 3 days time
 2. timing of next Halley's comet appearance
 3. time of sunrise this day next year
 4. Google stock price tomorrow
 5. Google stock price in 6 months time
 6. maximum temperature tomorrow
 7. exchange rate of \$US/AUS next week
 8. total sales of drugs in Australian pharmacies next month

\pause

 - how do we measure "easiest"?
 - what makes something easy/difficult to forecast?

## Factors affecting forecastability

Something is easier to forecast if:

 - we have a good understanding of the factors that contribute to it
 - there is lots of data available;
 - the forecasts cannot affect the thing we are trying to forecast.
 - there is relatively low natural/unexplainable random variation.
 - the future is somewhat similar to the past

## Random futures

```{r austa, echo=FALSE}
# Grab ABS data
austa <- readxl::read_excel("data/340101.xlsx", sheet = "Data1", skip = 9) |>
  rename(date = `Series ID`, value = A85375847A) |>
  select(date, value) |>
  transmute(
    Month = yearmonth(date),
    Visitors = value / 1e3
  ) |>
  bind_rows(tibble(
    Month = yearmonth(seq(as.Date("2021-11-01"), by = "1 month", length = 2)),
    Visitors = NA_real_
  )) |>
  as_tsibble(index = Month) |>
  filter(Month >= yearmonth("2000 Jan"))
# Fit ETS model
fit <- austa |>
  filter(Month < yearmonth("2018 Jan")) |>
  model(ETS(Visitors))
# Product forecasts
fc <- forecast(fit, h = 48) |>
  mutate(Month = as.Date(Month))
# Simulate 100 future sample paths
set.seed(1967)
sim <- fit |>
  generate(h = 48, times = 100) |>
  mutate(
    replicate = factor(.rep, levels = 1:100, labels = paste("Future", 1:100)),
    .rep = as.numeric(.rep)
  ) |>
  as_tibble() |>
  mutate(Month = as.Date(Month))
# Nice colors
cols <- scale_colour_manual(
  values = c("#D55E00", "#0072B2", "#009E73", "#CC79A7", "#E69F00", "#56B4E9", "#F0E442", sample(rainbow(93))),
  breaks = paste("Future", 1:100),
  name = " "
)
# Now build up some plots with alignment
p1 <- austa |>
  mutate(Visitors = if_else(Month >= yearmonth("2018 Jan"), NA_real_, Visitors)) |>
  as_tibble() |>
  mutate(Month = as.Date(Month)) |>
  ggplot(aes(x = Month, y = Visitors)) +
  geom_line() +
  labs(
    x = "Month",
    y = "Thousands of visitors",
    title = "Total short-term visitors to Australia"
  ) +
  scale_x_date(
    breaks = seq(as.Date("2000-01-01"), by = "5 years", l = 5),
    labels = paste("Jan", seq(2000, 2020, by = 5)),
    minor_breaks = seq(as.Date("2001-01-01"), by = "1 year", l = 25)
  ) +
  ylim(min(austa$Visitors, sim$.sim, na.rm = TRUE), max(austa$Visitors, sim$.sim, na.rm = TRUE))
p2 <- p1 + cols +
  geom_line(aes(y = .sim, group = replicate, colour = replicate),
    data = sim |> filter(.rep <= 1)
  )
p3 <- p1 + cols +
  geom_line(aes(y = .sim, group = replicate, colour = replicate),
    data = sim |> filter(.rep <= 2)
  )
p4 <- p1 + cols +
  geom_line(aes(y = .sim, group = replicate, colour = replicate),
    data = sim |> filter(.rep <= 3)
  )
p5 <- p1 + cols +
  geom_line(aes(y = .sim, group = replicate, colour = replicate),
    data = sim |> filter(.rep <= 10)
  )
p6 <- p1 + cols +
  geom_line(aes(y = .sim, group = replicate, colour = replicate), alpha = 0.5, data = sim) +
  guides(colour = "none")
p7 <- p1 +
  geom_line(aes(y = .sim, group = replicate, colour = replicate), alpha = 0.5, col = "gray", data = sim) +
  guides(colour = "none")
p8 <- p7 + autolayer(fc, level = c(50, 90))
p9 <- p8 + coord_cartesian(xlim = as.Date(c("2014-01-01", "2021-08-01"))) +
  scale_x_date(
    breaks = seq(as.Date("2000-01-01"), by = "2 years", l = 11),
    labels = paste("Jan", seq(2000, 2020, by = 2)),
    minor_breaks = seq(as.Date("2001-01-01"), by = "1 year", l = 25)
  )
aligned_plots <- align_patches(p1, p2, p3, p4, p5, p6, p7, p8, p9)
```

\forecast\pause

```{r austa1, dependson="austa", echo=FALSE}
aligned_plots[[1]]
```

## Random futures

\forecast

```{r austa2, dependson='austa', echo=FALSE}
aligned_plots[[2]]
```

\simfutures

## Random futures

\forecast

```{r austa3, dependson='austa', echo=FALSE}
aligned_plots[[3]]
```

\simfutures

## Random futures

\forecast

```{r austa4, dependson='austa', echo=FALSE}
aligned_plots[[4]]
```

\simfutures

## Random futures

\forecast

```{r austa5, dependson='austa', echo=FALSE}
aligned_plots[[5]]
```

\simfutures

## Random futures

\forecast

```{r austa6, dependson='austa', echo=FALSE}
aligned_plots[[6]]
```

\simfutures

## Random futures

\forecast

```{r austa7, dependson='austa', echo=FALSE}
aligned_plots[[7]]
```

\simfutures

## Random futures

\forecast

```{r austa8, dependson='austa', echo=FALSE}
aligned_plots[[8]]
```

\simfutures

## Random futures

\forecast

```{r austa9, dependson='austa', echo=FALSE}
aligned_plots[[9]]
```

\simfutures

## Random futures

\forecast

```{r austa9b, dependson='austa', echo=FALSE}
aligned_plots[[9]] +
  geom_line(
    colour = "black",
    data = austa |> filter(Month >= yearmonth("2018 Jan")) |> mutate(Month = as.Date(Month))
  )
```

\simfutures

\only<2>{\begin{textblock}{8.5}(.3,6.5)\begin{alertblock}{}
``He who sees the past as surprise-free is bound to have a future full of surprises.''\\\mbox{}\hfill{\small (Amos Tversky)}
\end{alertblock}\end{textblock}
}

## Statistical forecasting

- Thing to be forecast: $y_{T+h}$.
- What we know: $y_1,\dots,y_T$.
- Forecast distribution: ${y}_{T+h|t} = y_{T+h} \mid \{y_1,y_2,\dots,y_{T}\}$.
- Point forecast: $\hat{y}_{T+h|T} =\text{E}[y_{T+h} \mid y_1,\dots,y_T]$.
- Forecast variance: $\text{Var}[y_{t}  \mid y_1,\dots,y_T]$
- Prediction interval is a range of values of $y_{T+h}$ with high probability.

# Benchmark methods

## Some simple forecasting methods

```{r ausbeer, fig.height=4.6, echo=FALSE}
new_production <- aus_production |>
  filter(year(Quarter) >= 1992)
new_production |> autoplot(Beer) +
  labs(
    x = "Year", y = "megalitres",
    title = "Australian quarterly beer production"
  )
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these series?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods

```{r pigs, fig.height=4.6, echo=FALSE}
aus_livestock |>
  filter(
    between(year(Month), 1992, 1996),
    Animal == "Pigs", State == "Victoria"
  ) |>
  autoplot(Count) +
  labs(
    x = "Year", y = "thousands",
    title = "Number of pigs slaughtered in Victoria, 1990-1995"
  )
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these series?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods

```{r dj, fig.height=4.6, echo=FALSE}
gafa_stock |>
  filter(Symbol == "FB", Date >= ymd("2018-01-01")) |>
  autoplot(Close) +
  labs(
    title = "Facebook closing stock price in 2018",
    x = NULL, y = "Closing price ($USD)"
  )
```

\begin{textblock}{7}(0.2,8.7)
\begin{alertblock}{}
\small{How would you forecast these series?}
\end{alertblock}
\end{textblock}

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `MEAN(y)`: Average method

  * Forecast of all future values is equal to mean of historical data $\{y_1,\dots,y_T\}$.
  * Forecasts: $\hat{y}_{T+h|T} = \bar{y} = (y_1+\dots+y_T)/T$

```{r mean-method-explained, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 2.7}
bricks <- aus_production |>
  filter(!is.na(Bricks)) |>
  mutate(average = mean(Bricks))

fc <- bricks |>
  model(MEAN(Bricks)) |>
  forecast(h = "5 years")

bricks |>
  ggplot(aes(x = Quarter, y = Bricks)) +
  geom_line() +
  geom_line(aes(y = average), colour = "blue", linetype = "dashed") +
  geom_line(aes(y = .mean), data = fc, colour = "blue") +
  labs(title = "Clay brick production in Australia")
```

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `NAIVE(y)`: Naïve method

  * Forecasts equal to last observed value.
  * Forecasts: $\hat{y}_{T+h|T} =y_T$.
  * Consequence of efficient market hypothesis.

```{r naive-method-explained, echo = FALSE, warning = FALSE, fig.height = 2.7}
bricks |>
  filter(!is.na(Bricks)) |>
  model(NAIVE(Bricks)) |>
  forecast(h = "5 years") |>
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(aes(y = Bricks), data = slice(bricks, n()), colour = "blue") +
  labs(title = "Clay brick production in Australia")
```

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `SNAIVE(y ~ lag(m))`: Seasonal naïve method

  * Forecasts equal to last value from same season.
  * Forecasts: $\hat{y}_{T+h|T} =y_{T+h-m(k+1)}$, where $m=$ seasonal period and $k$ is the integer part of $(h-1)/m$.

```{r snaive-method-explained, echo = FALSE, warning = FALSE, fig.height = 2.7}
bricks |>
  model(SNAIVE(Bricks ~ lag("year"))) |>
  forecast(h = "5 years") |>
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(aes(y = Bricks), data = slice(bricks, (n() - 3):n()), colour = "blue") +
  labs(title = "Clay brick production in Australia")
```

## Some simple forecasting methods
\fontsize{13}{14}\sf

### `RW(y ~ drift())`: Drift method

 * Forecasts equal to last value plus average change.
 * Forecasts:\vspace*{-.7cm}

 \begin{align*}
 \hat{y}_{T+h|T} & =  y_{T} + \frac{h}{T-1}\sum_{t=2}^T (y_t-y_{t-1})\\
                 & = y_T + \frac{h}{T-1}(y_T -y_1).
 \end{align*}\vspace*{-0.2cm}

   * Equivalent to extrapolating a line drawn between first and last observations.

## Some simple forecasting methods

### Drift method

```{r drift-method-explained, echo = FALSE, warning = FALSE}
aus_production |>
  filter(!is.na(Bricks)) |>
  model(RW(Bricks ~ drift())) |>
  forecast(h = "5 years") |>
  autoplot(aus_production, level = NULL) +
  geom_line(
    aes(y = Bricks),
    data = slice(aus_production, range(cumsum(!is.na(Bricks)))),
    linetype = "dashed", colour = "blue"
  ) +
  labs(title = "Clay brick production in Australia")
```

## Model fitting

The `model()` function trains models to data.

\fontsize{10}{11}\sf

```{r brick-model}
brick_fit <- aus_production |>
  filter(!is.na(Bricks)) |>
  model(
    `Seasonal_naïve` = SNAIVE(Bricks),
    `Naïve` = NAIVE(Bricks),
    Drift = RW(Bricks ~ drift()),
    Mean = MEAN(Bricks)
  )
```

```{r brick-model2, echo=FALSE, dependson='brick-model'}
brick_fit
```

\vspace*{-0.2cm}\begin{alertblock}{}
A \texttt{mable} is a model table, each cell corresponds to a fitted model.
\end{alertblock}

## Producing forecasts

\fontsize{10}{13}\sf

```{r brick-fc, echo = TRUE, dependson='brick-model'}
brick_fc <- brick_fit |>
  forecast(h = "5 years")
```

```{r brick-fbl, echo = FALSE, dependson='brick-fc'}
print(brick_fc, n = 4)
```

\begin{alertblock}{}
A \texttt{fable} is a forecast table with point forecasts and distributions.
\end{alertblock}

## Visualising forecasts

\footnotesize

```{r brick-fc-plot, warning=FALSE, message=FALSE, fig.height=2.6, dependson='brick-fc'}
brick_fc |>
  autoplot(aus_production, level = NULL) +
  labs(title = "Forecasts for quarterly clay brick production",
       x = "Year", y = "Millions of bricks") +
  guides(colour = guide_legend(title = "Forecast"))
```

## Prediction intervals
\fontsize{10}{12}\sf

```{r brick-fc-interval, dependson='brick-fc'}
brick_fc |> hilo(level = c(50, 75))
```

## Prediction intervals
\fontsize{10}{12}\sf

```{r brick-fc-interval2, dependson='brick-fc'}
brick_fc |>
  hilo(level = c(50, 75)) |>
  unpack_hilo(c("50%", "75%"))
```

# Lab Session 11
## Lab Session 11

 * Produce forecasts using an appropriate benchmark method for household wealth (`hh_budget`). Plot the results using `autoplot()`.
 * Produce forecasts using an appropriate benchmark method for Australian takeaway food turnover (`aus_retail`). Plot the results using `autoplot()`.

# Residual diagnostics

## Fitted values

 - $\hat{y}_{t|t-1}$ is the forecast of $y_t$ based on observations $y_1,\dots,y_t$.
 - We call these "fitted values".
 - Sometimes drop the subscript: $\hat{y}_t \equiv \hat{y}_{t|t-1}$.
 - Often not true forecasts since parameters are estimated on all data.

### For example:

 - $\hat{y}_{t} = \bar{y}$ for average method.
 - $\hat{y}_{t} = y_{t-1} + (y_{T}-y_1)/(T-1)$ for drift method.

## Forecasting residuals

\begin{block}{}
\textbf{Residuals in forecasting:} difference between observed value and its fitted value: $e_t = y_t-\hat{y}_{t|t-1}$.
\end{block}
\pause\fontsize{13}{15}\sf

\alert{Assumptions}

  1. $\{e_t\}$ uncorrelated. If they aren't, then information left in  residuals that should be used in computing forecasts.
  2. $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

\pause

\alert{Useful properties} (for prediction intervals)

  3. $\{e_t\}$ have constant variance.
  4. $\{e_t\}$ are normally distributed.

## Facebook closing stock price
\fontsize{9}{10}\sf

```{r fbf}
fb_stock <- gafa_stock |>
  filter(Symbol == "FB")
fb_stock |> autoplot(Close)
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r augment}
fb_stock <- fb_stock |>
  mutate(trading_day = row_number()) |>
  update_tsibble(index = trading_day, regular = TRUE)
fit <- fb_stock |> model(NAIVE(Close))
augment(fit)
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj4, echo=TRUE, warning=FALSE, fig.height=2.5, dependson="augment"}
augment(fit) |>
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj4a, echo=TRUE, warning=FALSE, fig.height=2.5, dependson="augment"}
augment(fit) |>
  filter(trading_day > 1100) |>
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

## Facebook closing stock price
\fontsize{10}{10}\sf

```{r dj5, echo=TRUE, warning = FALSE, dependson="augment"}
augment(fit) |>
  autoplot(.resid) +
  labs(x = "Day", y = "", title = "Residuals from naïve method")
```

## Facebook closing stock price
\fontsize{11}{11}\sf

```{r dj6, warning=FALSE, fig.height=2.7, dependson="augment"}
augment(fit) |>
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 150) +
  labs(title = "Histogram of residuals")
```

## Facebook closing stock price
\fontsize{11}{11}\sf

```{r dj7, dependson="augment"}
augment(fit) |>
  ACF(.resid) |>
  autoplot() + labs(title = "ACF of residuals")
```

## ACF of residuals

  * We assume that the residuals are white noise (uncorrelated, mean zero, constant variance). If they aren't, then there is information left in  the residuals that should be used in computing forecasts.

  * So a standard residual diagnostic is to check the ACF of the residuals of a forecasting method.

  * We *expect* these to look like white noise.

## Combined diagnostic graph
\fontsize{11}{11}\sf

```{r dj8, dependson="augment"}
fit |> gg_tsresiduals()
```

## Ljung-Box test
\fontsize{13}{15}\sf

Test whether *whole set* of $r_{k}$ values is significantly different from zero set.

\begin{block}{}
\centerline{$\displaystyle
 Q = T(T+2) \sum_{k=1}^h (T-k)^{-1}r_k^2$\qquad
where $h=$ max lag and $T=$ \# observations.}
\end{block}

  * If each $r_k$ close to zero, $Q$ will be **small**.
  * If some $r_k$ values large ($+$ or $-$), $Q$ will be **large**.
  * My preferences: $h=10$ for non-seasonal data, $h=2m$ for seasonal data.
  * If data are WN, $Q \sim \chi^2$ with $(h - K)$ degrees of freedom where $K=$ no.\ parameters in model.
  * When applied to raw data, set $K=0$.

## Ljung-Box test
\fontsize{12}{13}\sf

\begin{block}{}
\centerline{$\displaystyle
 Q = T(T+2) \sum_{k=1}^h (T-k)^{-1}r_k^2$\qquad
where $h=$ max lag and $T=$ \# observations.}
\end{block}

\fontsize{11}{11}\sf

```{r dj9extra, echo=FALSE, fig.height=1.5}
augment(fit) |>
  ACF(.resid, lag_max = 10) |>
  autoplot() + labs(title = "ACF of residuals")
```

\vspace*{-0.3cm}

```{r dj9, echo=TRUE, dependson="augment"}
# lag=h and dof=K
augment(fit) |> features(.resid, ljung_box, dof = 0, lag = 10)
```

# Lab Session 12
## Lab Session 12

  * Compute seasonal naïve forecasts for quarterly Australian beer production.

  * Test if the residuals are white noise. What do you conclude?

# Forecast accuracy measures

## Training and test sets

```{r traintest, fig.height=1, echo=FALSE}
train <- 1:18
test <- 19:24
par(mar = c(0, 0, 0, 0))
plot(0, 0, xlim = c(0, 26), ylim = c(0, 2), xaxt = "n", yaxt = "n", bty = "n", xlab = "", ylab = "", type = "n")
arrows(0, 0.5, 25, 0.5, 0.05)
points(train, train * 0 + 0.5, pch = 19, col = "blue")
points(test, test * 0 + 0.5, pch = 19, col = "red")
text(26, 0.5, "time")
text(10, 1, "Training data", col = "blue")
text(21, 1, "Test data", col = "red")
```

  * A model which fits the training data well will not necessarily forecast well.
  * Forecast accuracy is based only on the test set.

### Forecast errors

Forecast "error": the difference between an observed value and its forecast.
$$
  e_{T+h} = y_{T+h} - \hat{y}_{T+h|T},
$$
where the training data is given by $\{y_1,\dots,y_T\}$

## Measures of forecast accuracy
\fontsize{11}{12}\sf

```r
beer_fit <- aus_production |>
  filter(between(year(Quarter), 1992, 2007)) |>
  model(
    snaive = SNAIVE(Beer),
    mean = MEAN(Beer)
  )
beer_fit |>
  forecast(h = "3 years") |>
  autoplot(aus_production, level = NULL) +
  labs(title ="Forecasts for quarterly beer production",
       x ="Year", y ="Megalitres") +
  guides(colour = guide_legend(title = "Forecast"))
```

## Measures of forecast accuracy

```{r beer-fc-1, echo=FALSE, fig.height=4}
beer_fit <- aus_production |>
  filter(between(year(Quarter), 1992, 2007)) |>
  model(
    snaive = SNAIVE(Beer),
    mean = MEAN(Beer)
  )
beer_fit |>
  forecast(h = "3 years") |>
  autoplot(aus_production, level = NULL) +
  labs(
    title = "Forecasts for quarterly beer production",
    x = "Year", y = "Megalitres"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

## Measures of forecast accuracy

\begin{tabular}{rl}
$y_{T+h}=$ & $(T+h)$th observation, $h=1,\dots,H$ \\
$\pred{y}{T+h}{T}=$ & its forecast based on data up to time $T$. \\
$e_{T+h} =$  & $y_{T+h} - \pred{y}{T+h}{T}$
\end{tabular}

\begin{align*}
\text{MAE} &= \text{mean}(|e_{T+h}|) \\[-0.2cm]
\text{MSE} &= \text{mean}(e_{T+h}^2) \qquad
&&\text{RMSE} &= \sqrt{\text{mean}(e_{T+h}^2)} \\[-0.1cm]
\text{MAPE} &= 100\text{mean}(|e_{T+h}|/ |y_{T+h}|)
\end{align*}\pause\vspace*{-0.4cm}

  * MAE, MSE, RMSE are all scale dependent.
  * MAPE is scale independent but is only sensible if $y_t\gg 0$ for all $t$, and $y$ has a natural zero.

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For non-seasonal time series,
$$
  Q = (T-1)^{-1}\sum_{t=2}^T |y_t-y_{t-1}|
$$
works well. Then MASE is equivalent to MAE relative to a naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

\begin{block}{Mean Absolute Scaled Error}
$$
\text{MASE} = \text{mean}(|e_{T+h}|/Q)
$$
where $Q$ is a stable measure of the scale of the time series $\{y_t\}$.
\end{block}
Proposed by Hyndman and Koehler (IJF, 2006).

For seasonal time series,
$$
  Q = (T-m)^{-1}\sum_{t=m+1}^T |y_t-y_{t-m}|
$$
works well. Then MASE is equivalent to MAE relative to a seasonal naïve method.

\vspace*{10cm}

## Measures of forecast accuracy

\fontsize{9.8}{10}\sf

```{r beer-test-accuracy, dependson='beer-fc-1'}
beer_fc <- forecast(beer_fit, h = "3 years")
accuracy(beer_fc, aus_production)
```

# Lab Session 13
## Lab Session 13

 * Create a training set for household wealth (`hh_budget`) by witholding the last four years as a test set.
 * Fit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.
 * Compute the accuracy of your forecasts. Which method does best?
 * Repeat the exercise using the Australian takeaway food turnover data (`aus_retail`) with a test set of four years.
